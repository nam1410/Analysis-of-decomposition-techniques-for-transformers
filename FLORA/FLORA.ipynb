{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfhlu66hrIep",
        "outputId": "d7189920-f643-479f-90cb-418d9c2103e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTYvON9NrAv2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSO7kqyurDB0"
      },
      "outputs": [],
      "source": [
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpBLn3IarGGK"
      },
      "outputs": [],
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWi6bTmGrNh9"
      },
      "outputs": [],
      "source": [
        "class LRLinearSuper(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, bias=True, fused=False, sample_ratio=1.0):\n",
        "        super().__init__()\n",
        "        self.bias = bias\n",
        "        self.fused = fused\n",
        "        self.sample_ratio = sample_ratio\n",
        "        self.num_components = min(in_channel, out_channel)\n",
        "        self.VT = nn.Linear(in_channel, int(round(self.num_components * sample_ratio)), bias=False)\n",
        "        self.U = nn.Linear(int(round(self.num_components * sample_ratio)), out_channel, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.fused:\n",
        "            weight = self.U.weight @ self.VT.weight\n",
        "            if self.bias:\n",
        "                return F.linear(x, weight, self.U.bias)\n",
        "            else:\n",
        "                return F.linear(x, weight)\n",
        "        else:\n",
        "            x = self.VT(x)\n",
        "            return self.U(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdWVev6urPa3"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0., ratio=0.5):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            LRLinearSuper(dim, hidden_dim, fused=True, sample_ratio=ratio),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            LRLinearSuper(hidden_dim, dim, fused=True, sample_ratio=ratio),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SGiDdRurQ_S"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0., ratio=0.5):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "        self.to_qkv = LRLinearSuper(dim, inner_dim * 3, fused=True, sample_ratio=ratio)\n",
        "        self.to_out = nn.Sequential(\n",
        "            LRLinearSuper(inner_dim, dim, fused=True, sample_ratio=ratio),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "        attn = self.attend(dots)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5S-6unPrSh4"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc2by-fgrVll"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool='cls', channels=3, dim_head=64, dropout=0., emb_dropout=0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        self.pool = pool\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        if self.pool == 'mean':\n",
        "            x = x.mean(dim=1)\n",
        "        elif self.pool == 'cls':\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY2MjNCmrvJP",
        "outputId": "a411f840-04d2-4a14-f6f7-ef3cd2eee393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(outputs, labels):\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    correct = (predictions == labels).type(torch.float).sum().item()\n",
        "    return correct / labels.size(0)"
      ],
      "metadata": {
        "id": "ig_8PPea6Yih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "8VH9coWxq_Af",
        "outputId": "1486ac6d-9c91-46fa-bdbe-2ddf938e0eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.2885, Accuracy: 0.1385\n",
            "Epoch 2, Loss: 2.1111, Accuracy: 0.1931\n",
            "Epoch 3, Loss: 2.0879, Accuracy: 0.1971\n",
            "Epoch 4, Loss: 2.1000, Accuracy: 0.1940\n",
            "Epoch 5, Loss: 2.1084, Accuracy: 0.1899\n",
            "Epoch 6, Loss: 2.0698, Accuracy: 0.2036\n",
            "Epoch 7, Loss: 2.0625, Accuracy: 0.2110\n",
            "Epoch 8, Loss: 2.0526, Accuracy: 0.2109\n",
            "Epoch 9, Loss: 2.0328, Accuracy: 0.2193\n",
            "Epoch 10, Loss: 2.0264, Accuracy: 0.2238\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-bb10b2792173>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ViT(\n",
        "    image_size=32,\n",
        "    patch_size=4,\n",
        "    num_classes=10,\n",
        "    dim=512,\n",
        "    depth=6,\n",
        "    heads=8,\n",
        "    mlp_dim=512,\n",
        "    dim_head=64,\n",
        "    dropout=0.1,\n",
        "    emb_dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "# Training loop with accuracy, gradient clipping, and LR scheduler\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_accuracy = 0\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_accuracy += (outputs.argmax(1) == labels).float().mean().item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_train_accuracy = train_accuracy / len(train_loader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {avg_train_loss:.4f}, Accuracy: {avg_train_accuracy:.4f}')\n",
        "\n",
        "# Testing loop for accuracy after training\n",
        "model.eval()\n",
        "test_accuracy = 0\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(imgs)\n",
        "        test_accuracy += (outputs.argmax(1) == labels).float().mean().item()\n",
        "\n",
        "avg_test_accuracy = test_accuracy / len(test_loader)\n",
        "print(f'Test Accuracy: {avg_test_accuracy:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}