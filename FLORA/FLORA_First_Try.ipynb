{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVw3W3YpRcPD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, num_classes, rank_config):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.rank_config = rank_config\n",
        "\n",
        "        # a basic ViT with self-attention\n",
        "        self.embedding = nn.Linear(3 * 32 * 32, 512)  # Flatten input image\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
        "            for _ in range(6)  # Number of transformer layers\n",
        "        ])\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        # we should create low-rank layers based on rank_config, For example: we can apply SVD-based decomposition to self.fc.weight\n",
        "        # rank can be adjusted based on rank_config\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flattening input image\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Embedding layer\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Classification head\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "rank_config = {\"rank\": 16}  # Customize rank as needed\n",
        "model = VisionTransformer(num_classes=10, rank_config=rank_config)"
      ],
      "metadata": {
        "id": "rNXN4PJmRuhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "#CIFAR-10 train and test dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Class labels\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ToSSJ6TRk7u",
        "outputId": "cec51a7e-b289-4667-c31c-faeb447647ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rank_config = {\n",
        "    'num_ranks': 4,                        # Number of ranks (e.g., 4 for 4 ranks)\n",
        "    'rank_candidates': [16, 32, 64, 128],  # List of candidate rank values\n",
        "    'filter_strategy': 'top_k',            # Candidate filtering strategy (e.g., 'top_k' or 'threshold')\n",
        "    'filter_value': 2                      # Value for filtering (e.g., top 2 candidates)\n",
        "}\n",
        "# Create FLORA supernet\n",
        "model = VisionTransformer(num_classes=1000, rank_config=rank_config)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "log_interval = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (images, labels) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{batch_idx+1}/{len(trainloader)}] Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3Ouk7NhRpO1",
        "outputId": "440dbefd-fa23-4aba-c8f0-176900c53833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Batch [1/782] Loss: 6.880486011505127\n",
            "Epoch [1/10] Batch [101/782] Loss: 2.2895030975341797\n",
            "Epoch [1/10] Batch [201/782] Loss: 2.364046812057495\n",
            "Epoch [1/10] Batch [301/782] Loss: 2.2933807373046875\n",
            "Epoch [1/10] Batch [401/782] Loss: 2.368079900741577\n",
            "Epoch [1/10] Batch [501/782] Loss: 2.407742500305176\n",
            "Epoch [1/10] Batch [601/782] Loss: 2.307753562927246\n",
            "Epoch [1/10] Batch [701/782] Loss: 2.3778953552246094\n",
            "Epoch [2/10] Batch [1/782] Loss: 2.355701208114624\n",
            "Epoch [2/10] Batch [101/782] Loss: 2.3230113983154297\n",
            "Epoch [2/10] Batch [201/782] Loss: 2.338007688522339\n",
            "Epoch [2/10] Batch [301/782] Loss: 2.318798542022705\n",
            "Epoch [2/10] Batch [401/782] Loss: 2.3363893032073975\n",
            "Epoch [2/10] Batch [501/782] Loss: 2.339705228805542\n",
            "Epoch [2/10] Batch [601/782] Loss: 2.3411717414855957\n",
            "Epoch [2/10] Batch [701/782] Loss: 2.3137009143829346\n",
            "Epoch [3/10] Batch [1/782] Loss: 2.2913360595703125\n",
            "Epoch [3/10] Batch [101/782] Loss: 2.311530590057373\n",
            "Epoch [3/10] Batch [201/782] Loss: 2.309331178665161\n",
            "Epoch [3/10] Batch [301/782] Loss: 2.326902389526367\n",
            "Epoch [3/10] Batch [401/782] Loss: 2.3027448654174805\n",
            "Epoch [3/10] Batch [501/782] Loss: 2.32232928276062\n",
            "Epoch [3/10] Batch [601/782] Loss: 2.3210816383361816\n",
            "Epoch [3/10] Batch [701/782] Loss: 2.3211638927459717\n",
            "Epoch [4/10] Batch [1/782] Loss: 2.315171003341675\n",
            "Epoch [4/10] Batch [101/782] Loss: 2.2958168983459473\n",
            "Epoch [4/10] Batch [201/782] Loss: 2.3264732360839844\n",
            "Epoch [4/10] Batch [301/782] Loss: 2.2828404903411865\n",
            "Epoch [4/10] Batch [401/782] Loss: 2.3032338619232178\n",
            "Epoch [4/10] Batch [501/782] Loss: 2.3126003742218018\n",
            "Epoch [4/10] Batch [601/782] Loss: 2.287384271621704\n",
            "Epoch [4/10] Batch [701/782] Loss: 2.319648027420044\n",
            "Epoch [5/10] Batch [1/782] Loss: 2.3305020332336426\n",
            "Epoch [5/10] Batch [101/782] Loss: 2.323810577392578\n",
            "Epoch [5/10] Batch [201/782] Loss: 2.30835223197937\n",
            "Epoch [5/10] Batch [301/782] Loss: 2.3289318084716797\n",
            "Epoch [5/10] Batch [401/782] Loss: 2.2920238971710205\n",
            "Epoch [5/10] Batch [501/782] Loss: 2.288799285888672\n",
            "Epoch [5/10] Batch [601/782] Loss: 2.320356845855713\n",
            "Epoch [5/10] Batch [701/782] Loss: 2.3199257850646973\n",
            "Epoch [6/10] Batch [1/782] Loss: 2.3042471408843994\n",
            "Epoch [6/10] Batch [101/782] Loss: 2.2951977252960205\n",
            "Epoch [6/10] Batch [201/782] Loss: 2.3114097118377686\n",
            "Epoch [6/10] Batch [301/782] Loss: 2.3101391792297363\n",
            "Epoch [6/10] Batch [401/782] Loss: 2.2954490184783936\n",
            "Epoch [6/10] Batch [501/782] Loss: 2.305936574935913\n",
            "Epoch [6/10] Batch [601/782] Loss: 2.299395799636841\n",
            "Epoch [6/10] Batch [701/782] Loss: 2.323728561401367\n",
            "Epoch [7/10] Batch [1/782] Loss: 2.286348342895508\n",
            "Epoch [7/10] Batch [101/782] Loss: 2.2945544719696045\n",
            "Epoch [7/10] Batch [201/782] Loss: 2.2920572757720947\n",
            "Epoch [7/10] Batch [301/782] Loss: 2.305147886276245\n",
            "Epoch [7/10] Batch [401/782] Loss: 2.298957586288452\n",
            "Epoch [7/10] Batch [501/782] Loss: 2.2952330112457275\n",
            "Epoch [7/10] Batch [601/782] Loss: 2.2832396030426025\n",
            "Epoch [7/10] Batch [701/782] Loss: 2.311075210571289\n",
            "Epoch [8/10] Batch [1/782] Loss: 2.2984490394592285\n",
            "Epoch [8/10] Batch [101/782] Loss: 2.3083689212799072\n",
            "Epoch [8/10] Batch [201/782] Loss: 2.2994372844696045\n",
            "Epoch [8/10] Batch [301/782] Loss: 2.3057212829589844\n",
            "Epoch [8/10] Batch [401/782] Loss: 2.3062312602996826\n",
            "Epoch [8/10] Batch [501/782] Loss: 2.3085265159606934\n",
            "Epoch [8/10] Batch [601/782] Loss: 2.2902770042419434\n",
            "Epoch [8/10] Batch [701/782] Loss: 2.3133811950683594\n",
            "Epoch [9/10] Batch [1/782] Loss: 2.3099279403686523\n",
            "Epoch [9/10] Batch [101/782] Loss: 2.2842519283294678\n",
            "Epoch [9/10] Batch [201/782] Loss: 2.3075647354125977\n",
            "Epoch [9/10] Batch [301/782] Loss: 2.3097753524780273\n",
            "Epoch [9/10] Batch [401/782] Loss: 2.299220561981201\n",
            "Epoch [9/10] Batch [501/782] Loss: 2.3068201541900635\n",
            "Epoch [9/10] Batch [601/782] Loss: 2.3110151290893555\n",
            "Epoch [9/10] Batch [701/782] Loss: 2.3240115642547607\n",
            "Epoch [10/10] Batch [1/782] Loss: 2.3061435222625732\n",
            "Epoch [10/10] Batch [101/782] Loss: 2.2966558933258057\n",
            "Epoch [10/10] Batch [201/782] Loss: 2.2993147373199463\n",
            "Epoch [10/10] Batch [301/782] Loss: 2.312757730484009\n",
            "Epoch [10/10] Batch [401/782] Loss: 2.3100061416625977\n",
            "Epoch [10/10] Batch [501/782] Loss: 2.299452066421509\n",
            "Epoch [10/10] Batch [601/782] Loss: 2.308135509490967\n",
            "Epoch [10/10] Batch [701/782] Loss: 2.3106610774993896\n"
          ]
        }
      ]
    }
  ]
}