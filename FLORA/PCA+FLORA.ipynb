{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K19KwF8cRZZx",
        "outputId": "b19de34c-154c-47bc-8d6f-45b0ab724341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19vj0gxuO4wc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.nn.parameter import Parameter\n",
        "from einops import rearrange,repeat\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch.nn.functional as F\n",
        "from torch import optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg7G3gqDO6XH"
      },
      "outputs": [],
      "source": [
        "# helpers\n",
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKmYOVOOU0ij"
      },
      "outputs": [],
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGaOsWbK6uSQ"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, PCAAttention(dim, heads=heads, dim_head=dim_head, dropout=dropout, pca_components=16)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
        "            ]))\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZkiD1LDUq1i"
      },
      "outputs": [],
      "source": [
        "class LRLinearSuper(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, bias=True, fused=False, sample_ratio=1.0):\n",
        "        super().__init__()\n",
        "        self.bias = bias\n",
        "        self.fused = fused\n",
        "        self.sample_ratio = sample_ratio\n",
        "        self.num_components = min(in_channel, out_channel)\n",
        "        self.VT = nn.Linear(in_channel, int(round(self.num_components * sample_ratio)), bias=False)\n",
        "        self.U = nn.Linear(int(round(self.num_components * sample_ratio)), out_channel, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.fused:\n",
        "            weight = self.U.weight @ self.VT.weight\n",
        "            if self.bias:\n",
        "                return F.linear(x, weight, self.U.bias)\n",
        "            else:\n",
        "                return F.linear(x, weight)\n",
        "        else:\n",
        "            x = self.VT(x)\n",
        "            return self.U(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_hD8aW_O-ZJ"
      },
      "outputs": [],
      "source": [
        "# PCA-based Low Rank Linear Layer\n",
        "class PCALinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, rank):\n",
        "        super(PCALinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.rank = rank\n",
        "\n",
        "        # Initialize weights and bias\n",
        "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.bias = Parameter(torch.Tensor(out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "        # PCA components (initialized as None)\n",
        "        self.components = None\n",
        "        self.singular_values = None\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def perform_pca(self):\n",
        "        # Perform PCA on the detached weight matrix\n",
        "        pca = PCA(n_components=self.rank)\n",
        "        flattened_weights = self.weight.detach().cpu().numpy().reshape(self.out_features, -1)\n",
        "        pca.fit(flattened_weights)\n",
        "        self.components = torch.tensor(pca.components_, dtype=torch.float32, device=self.weight.device)\n",
        "        self.singular_values = torch.tensor(pca.singular_values_, dtype=torch.float32, device=self.weight.device)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.components is None or self.singular_values is None:\n",
        "            self.perform_pca()\n",
        "\n",
        "        # Project the input using PCA components\n",
        "        transformed_input = input.matmul(self.components.T) * self.singular_values\n",
        "        return torch.nn.functional.linear(transformed_input, self.components, self.bias)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return 'in_features={}, out_features={}, rank={}, bias={}'.format(\n",
        "            self.in_features, self.out_features, self.rank, self.bias is not None\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pRyivWHPACw"
      },
      "outputs": [],
      "source": [
        "# Pre-norm layer\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGET4zKPPBxH"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0., ratio=0.5):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            LRLinearSuper(dim, hidden_dim, fused=True, sample_ratio=ratio),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            LRLinearSuper(hidden_dim, dim, fused=True, sample_ratio=ratio),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hy0ppiRPEcD"
      },
      "outputs": [],
      "source": [
        "class PCAAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0., ratio=0.5, pca_components=32):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.dim_head = dim_head\n",
        "        inner_dim = dim_head * heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "\n",
        "        self.to_q = LRLinearSuper(dim, inner_dim, fused=True, sample_ratio=ratio)\n",
        "        self.to_k = LRLinearSuper(dim, inner_dim, fused=True, sample_ratio=ratio)\n",
        "        self.to_v = LRLinearSuper(dim, inner_dim, fused=True, sample_ratio=ratio)\n",
        "\n",
        "        self.compress_k = nn.Linear(inner_dim, pca_components, bias=False)\n",
        "        self.expand_k = nn.Linear(pca_components, inner_dim, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            LRLinearSuper(inner_dim, dim, fused=True, sample_ratio=ratio),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.to_q(x)\n",
        "        k = self.to_k(x)\n",
        "        v = self.to_v(x)\n",
        "\n",
        "        # Make sure the rearrangement matches the inner_dim\n",
        "        #print(\"Shape of q, k, v before rearrange:\", q.shape, k.shape, v.shape)\n",
        "        q = rearrange(q, 'b n (h d) -> b h n d', h=self.heads, d=self.dim_head)\n",
        "        k = rearrange(k, 'b n (h d) -> b h n d', h=self.heads, d=self.dim_head)\n",
        "        v = rearrange(v, 'b n (h d) -> b h n d', h=self.heads, d=self.dim_head)\n",
        "        #print(\"Shape of q, k, v after rearrange:\", q.shape, k.shape, v.shape)\n",
        "\n",
        "        # Make sure compression is done correctly\n",
        "        k_reshaped = rearrange(k, 'b h n d -> b n (h d)')\n",
        "        #print(\"Shape of k before compress:\", k_reshaped.shape)\n",
        "        k_compressed = self.compress_k(k_reshaped)\n",
        "        k_expanded = self.expand_k(k_compressed)\n",
        "        #print(\"Shape of k after expansion:\", k_expanded.shape)\n",
        "\n",
        "        # Rearrange to fit q\n",
        "        k_expanded = rearrange(k_expanded, 'b n (h d) -> b h n d', h=self.heads, d=self.dim_head)\n",
        "        dots = torch.matmul(q, k_expanded.transpose(-1, -2)) * self.scale\n",
        "        attn = self.attend(dots)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        #print(\"Shape of output after attention:\", out.shape)\n",
        "\n",
        "        return self.to_out(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_vq-blzOvpE"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool='cls', channels=3, dim_head=64, dropout=0., emb_dropout=0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        self.pool = pool  # Ensure pool is defined\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        if self.pool == 'mean':\n",
        "            x = x.mean(dim=1)\n",
        "        elif self.pool == 'cls':\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTQqh8rMPNCu",
        "outputId": "cedefdc7-c3a5-4b9a-938c-d1284489307f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)  # Adjusted number of workers\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn8Mtk6oVjm6"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(outputs, labels):\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    correct = (predictions == labels).type(torch.float).sum().item()\n",
        "    return correct / labels.size(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTSYOGKKPPRP",
        "outputId": "bbbbc1f8-ff2a-4f9f-9169-29a94d27ca7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.3330, Accuracy: 0.1230\n",
            "Epoch 2, Loss: 2.2178, Accuracy: 0.1630\n",
            "Epoch 3, Loss: 2.2069, Accuracy: 0.1661\n",
            "Epoch 4, Loss: 2.1975, Accuracy: 0.1662\n",
            "Epoch 5, Loss: 2.1605, Accuracy: 0.1801\n",
            "Epoch 6, Loss: 2.2352, Accuracy: 0.1480\n",
            "Epoch 7, Loss: 2.2818, Accuracy: 0.1306\n",
            "Epoch 8, Loss: 2.2685, Accuracy: 0.1348\n",
            "Epoch 9, Loss: 2.2546, Accuracy: 0.1421\n",
            "Epoch 10, Loss: 2.2622, Accuracy: 0.1367\n",
            "Test Accuracy: 0.1436\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ViT(\n",
        "    image_size = 32,\n",
        "    patch_size = 4,\n",
        "    num_classes = 10,\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    mlp_dim = 512,\n",
        "    dim_head = 64,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
        "# Training loop with accuracy\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_accuracy = 0\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_accuracy += compute_accuracy(outputs, labels)\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_train_accuracy = train_accuracy / len(train_loader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {avg_train_loss:.4f}, Accuracy: {avg_train_accuracy:.4f}')\n",
        "\n",
        "# Testing loop for accuracy after training\n",
        "model.eval()\n",
        "test_accuracy = 0\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(imgs)\n",
        "        test_accuracy += compute_accuracy(outputs, labels)\n",
        "\n",
        "avg_test_accuracy = test_accuracy / len(test_loader)\n",
        "print(f'Test Accuracy: {avg_test_accuracy:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}