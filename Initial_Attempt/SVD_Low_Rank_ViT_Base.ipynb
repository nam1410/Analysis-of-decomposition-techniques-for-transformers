{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oHFyx-jtio0",
        "outputId": "7ac03064-9f66-43e9-fe6d-88bb62e35be7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.16)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.17.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.4.127)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "!pip install timm\n",
        "import timm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9CrXoNXuKCh",
        "outputId": "153b9729-5c00-40f7-ba4b-0175b2e3104b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size=(16, 16)),  # Resize to handle input size for ViT\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))  # CIFAR-10 normalization\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers = 2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npA2bL2NtezJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SVDLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, rank=None):\n",
        "        super(SVDLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.rank = rank or min(in_features, out_features)\n",
        "\n",
        "        # Initialize weight and perform SVD\n",
        "        init_weight = torch.randn(out_features, in_features)\n",
        "        U, S, V = torch.svd(init_weight)\n",
        "\n",
        "        # Reduce to desired rank by truncating SVD components\n",
        "        self.U = nn.Parameter(U[:, :self.rank])\n",
        "        self.S = nn.Parameter(torch.diag(S[:self.rank]))\n",
        "        self.V = nn.Parameter(V[:, :self.rank])\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.U @ self.S @ self.V.t()\n",
        "        return x @ weight.t()\n",
        "\n",
        "\n",
        "\n",
        "class LowRankLinear(nn.Module):\n",
        "    \"\"\" Low-rank linear layer using two smaller dense layers. \"\"\"\n",
        "    def __init__(self, in_features, out_features, rank):\n",
        "        super().__init__()\n",
        "        self.rank = rank\n",
        "        self.linear1 = nn.Linear(in_features, rank)\n",
        "        self.linear2 = nn.Linear(rank, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.linear1(x))\n",
        "\n",
        "class EfficientAttention(nn.Module):\n",
        "    \"\"\" Efficient Attention with low-rank keys and values. \"\"\"\n",
        "    def __init__(self, dim, rank, num_heads=1, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.rank = rank\n",
        "        self.head_dim = rank // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.query = nn.Linear(dim, rank, bias=qkv_bias)\n",
        "        self.key = SVDLinear(dim, rank, rank//2)\n",
        "        self.value = SVDLinear(dim, rank, rank//2)\n",
        "        self.proj = nn.Linear(rank, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, _ = x.shape\n",
        "        q = self.query(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        k = self.key(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        v = self.value(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, self.rank)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "class EncodingFLORA(nn.Module):\n",
        "    \"\"\" Encoding block with low-rank attention and feed-forward network. \"\"\"\n",
        "    def __init__(self, dim, num_heads=1, rank=128, hidden_mul=4, qkv_bias=False, act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = EfficientAttention(dim, rank, num_heads, qkv_bias)\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            SVDLinear(dim, int(dim * hidden_mul), rank),\n",
        "            act_layer(),\n",
        "            nn.Linear(int(dim * hidden_mul), dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Bwa-s52u3TQ",
        "outputId": "4bbae53b-0ee8-41c1-e5d6-7c759d95e92b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "device = get_device()\n",
        "print(device)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZoXZ-H_1jm2"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzF01dPdui1p"
      },
      "outputs": [],
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.svd_linear1 = SVDLinear(768, 1024, rank=512)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.svd_linear2 = SVDLinear(1024, 10, rank=512)  # Output for 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 16*16*3)  # Flatten the image\n",
        "        x = self.svd_linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.svd_linear2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleNet()\n",
        "model.apply(init_weights)\n",
        "model.to(device)\n",
        "if next(model.parameters()).is_cuda:\n",
        "  print(\"model moved to cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDffnSHP1wbb"
      },
      "outputs": [],
      "source": [
        "# Example check for NaN or Inf in inputs and labels\n",
        "for inputs, labels in train_loader:\n",
        "    assert not torch.isnan(inputs).any(), \"Input has NaN values\"\n",
        "    assert not torch.isinf(inputs).any(), \"Input has Inf values\"\n",
        "    assert not torch.isnan(labels).any(), \"Labels have NaN values\"\n",
        "    assert not torch.isinf(labels).any(), \"Labels have Inf values\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRcukkI0vBap",
        "outputId": "6441fef8-bb4e-4fc2-a234-a63c73d546ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch[1/100], Average Loss:  3.062,Time Taken: 73.56437516212463 seconds.\n",
            "\n",
            "Epoch[2/100], Average Loss:  2.410,Time Taken: 70.67940974235535 seconds.\n",
            "\n",
            "Epoch[3/100], Average Loss:  1.996,Time Taken: 71.55696415901184 seconds.\n",
            "\n",
            "Epoch[4/100], Average Loss:  1.699,Time Taken: 72.88640666007996 seconds.\n",
            "\n",
            "Epoch[5/100], Average Loss:  1.411,Time Taken: 71.46317791938782 seconds.\n",
            "\n",
            "Epoch[6/100], Average Loss:  1.263,Time Taken: 70.38732886314392 seconds.\n",
            "\n",
            "Epoch[7/100], Average Loss:  1.073,Time Taken: 72.63445496559143 seconds.\n",
            "\n",
            "Epoch[8/100], Average Loss:  0.975,Time Taken: 70.15279912948608 seconds.\n",
            "\n",
            "Epoch[9/100], Average Loss:  0.859,Time Taken: 71.39410018920898 seconds.\n",
            "\n",
            "Epoch[10/100], Average Loss:  0.783,Time Taken: 71.56018471717834 seconds.\n",
            "\n",
            "Epoch[11/100], Average Loss:  0.721,Time Taken: 70.8158278465271 seconds.\n",
            "\n",
            "Epoch[12/100], Average Loss:  0.673,Time Taken: 70.0799458026886 seconds.\n",
            "\n",
            "Epoch[13/100], Average Loss:  0.636,Time Taken: 72.45615673065186 seconds.\n",
            "\n",
            "Epoch[14/100], Average Loss:  0.581,Time Taken: 70.56626868247986 seconds.\n",
            "\n",
            "Epoch[15/100], Average Loss:  0.556,Time Taken: 70.6716034412384 seconds.\n",
            "\n",
            "Epoch[16/100], Average Loss:  0.517,Time Taken: 71.17055678367615 seconds.\n",
            "\n",
            "Epoch[17/100], Average Loss:  0.485,Time Taken: 70.86226534843445 seconds.\n",
            "\n",
            "Epoch[18/100], Average Loss:  0.477,Time Taken: 69.94070243835449 seconds.\n",
            "\n",
            "Epoch[19/100], Average Loss:  0.448,Time Taken: 72.15737414360046 seconds.\n",
            "\n",
            "Epoch[20/100], Average Loss:  0.440,Time Taken: 69.99002981185913 seconds.\n",
            "\n",
            "Epoch[21/100], Average Loss:  0.415,Time Taken: 71.48195004463196 seconds.\n",
            "\n",
            "Epoch[22/100], Average Loss:  0.399,Time Taken: 71.08370447158813 seconds.\n",
            "\n",
            "Epoch[23/100], Average Loss:  0.384,Time Taken: 71.13482713699341 seconds.\n",
            "\n",
            "Epoch[24/100], Average Loss:  0.374,Time Taken: 70.04104447364807 seconds.\n",
            "\n",
            "Epoch[25/100], Average Loss:  0.363,Time Taken: 72.06728172302246 seconds.\n",
            "\n",
            "Epoch[26/100], Average Loss:  0.348,Time Taken: 70.12162971496582 seconds.\n",
            "\n",
            "Epoch[27/100], Average Loss:  0.336,Time Taken: 71.04205083847046 seconds.\n",
            "\n",
            "Epoch[28/100], Average Loss:  0.325,Time Taken: 71.31494665145874 seconds.\n",
            "\n",
            "Epoch[29/100], Average Loss:  0.317,Time Taken: 71.18466877937317 seconds.\n",
            "\n",
            "Epoch[30/100], Average Loss:  0.303,Time Taken: 69.75691533088684 seconds.\n",
            "\n",
            "Epoch[31/100], Average Loss:  0.304,Time Taken: 72.14646530151367 seconds.\n",
            "\n",
            "Epoch[32/100], Average Loss:  0.293,Time Taken: 70.07605791091919 seconds.\n",
            "\n",
            "Epoch[33/100], Average Loss:  0.281,Time Taken: 70.86950159072876 seconds.\n",
            "\n",
            "Epoch[34/100], Average Loss:  0.273,Time Taken: 70.74081230163574 seconds.\n",
            "\n",
            "Epoch[35/100], Average Loss:  0.276,Time Taken: 70.60077834129333 seconds.\n",
            "\n",
            "Epoch[36/100], Average Loss:  0.256,Time Taken: 69.29171013832092 seconds.\n",
            "\n",
            "Epoch[37/100], Average Loss:  0.256,Time Taken: 71.60840940475464 seconds.\n",
            "\n",
            "Epoch[38/100], Average Loss:  0.253,Time Taken: 69.16811084747314 seconds.\n",
            "\n",
            "Epoch[39/100], Average Loss:  0.234,Time Taken: 69.38187766075134 seconds.\n",
            "\n",
            "Epoch[40/100], Average Loss:  0.236,Time Taken: 71.48911380767822 seconds.\n",
            "\n",
            "Epoch[41/100], Average Loss:  0.234,Time Taken: 69.60724687576294 seconds.\n",
            "\n",
            "Epoch[42/100], Average Loss:  0.221,Time Taken: 72.02673363685608 seconds.\n",
            "\n",
            "Epoch[43/100], Average Loss:  0.220,Time Taken: 69.80715751647949 seconds.\n",
            "\n",
            "Epoch[44/100], Average Loss:  0.225,Time Taken: 70.57847833633423 seconds.\n",
            "\n",
            "Epoch[45/100], Average Loss:  0.220,Time Taken: 70.64455080032349 seconds.\n",
            "\n",
            "Epoch[46/100], Average Loss:  0.216,Time Taken: 73.84916830062866 seconds.\n",
            "\n",
            "Epoch[47/100], Average Loss:  0.213,Time Taken: 68.43339109420776 seconds.\n",
            "\n",
            "Epoch[48/100], Average Loss:  0.201,Time Taken: 70.37415766716003 seconds.\n",
            "\n",
            "Epoch[49/100], Average Loss:  0.205,Time Taken: 68.52969908714294 seconds.\n",
            "\n",
            "Epoch[50/100], Average Loss:  0.196,Time Taken: 69.27311062812805 seconds.\n",
            "\n",
            "Epoch[51/100], Average Loss:  0.194,Time Taken: 69.4501416683197 seconds.\n",
            "\n",
            "Epoch[52/100], Average Loss:  0.184,Time Taken: 69.69403982162476 seconds.\n",
            "\n",
            "Epoch[53/100], Average Loss:  0.192,Time Taken: 68.32184338569641 seconds.\n",
            "\n",
            "Epoch[54/100], Average Loss:  0.194,Time Taken: 70.36878681182861 seconds.\n",
            "\n",
            "Epoch[55/100], Average Loss:  0.174,Time Taken: 68.26976108551025 seconds.\n",
            "\n",
            "Epoch[56/100], Average Loss:  0.171,Time Taken: 69.23931050300598 seconds.\n",
            "\n",
            "Epoch[57/100], Average Loss:  0.172,Time Taken: 69.2491602897644 seconds.\n",
            "\n",
            "Epoch[58/100], Average Loss:  0.178,Time Taken: 69.1928768157959 seconds.\n",
            "\n",
            "Epoch[59/100], Average Loss:  0.164,Time Taken: 68.15741801261902 seconds.\n",
            "\n",
            "Epoch[60/100], Average Loss:  0.172,Time Taken: 70.08729386329651 seconds.\n",
            "\n",
            "Epoch[61/100], Average Loss:  0.167,Time Taken: 68.19132232666016 seconds.\n",
            "\n",
            "Epoch[62/100], Average Loss:  0.162,Time Taken: 69.28039693832397 seconds.\n",
            "\n",
            "Epoch[63/100], Average Loss:  0.167,Time Taken: 69.2488284111023 seconds.\n",
            "\n",
            "Epoch[64/100], Average Loss:  0.154,Time Taken: 69.14338707923889 seconds.\n",
            "\n",
            "Epoch[65/100], Average Loss:  0.160,Time Taken: 68.04225897789001 seconds.\n",
            "\n",
            "Epoch[66/100], Average Loss:  0.161,Time Taken: 70.24609351158142 seconds.\n",
            "\n",
            "Epoch[67/100], Average Loss:  0.154,Time Taken: 67.98590612411499 seconds.\n",
            "\n",
            "Epoch[68/100], Average Loss:  0.156,Time Taken: 70.1329083442688 seconds.\n",
            "\n",
            "Epoch[69/100], Average Loss:  0.156,Time Taken: 68.35956597328186 seconds.\n",
            "\n",
            "Epoch[70/100], Average Loss:  0.150,Time Taken: 70.0452470779419 seconds.\n",
            "\n",
            "Epoch[71/100], Average Loss:  0.151,Time Taken: 69.33860969543457 seconds.\n",
            "\n",
            "Epoch[72/100], Average Loss:  0.145,Time Taken: 70.51838183403015 seconds.\n",
            "\n",
            "Epoch[73/100], Average Loss:  0.152,Time Taken: 68.53650879859924 seconds.\n",
            "\n",
            "Epoch[74/100], Average Loss:  0.142,Time Taken: 70.6004250049591 seconds.\n",
            "\n",
            "Epoch[75/100], Average Loss:  0.139,Time Taken: 69.78605127334595 seconds.\n",
            "\n",
            "Epoch[76/100], Average Loss:  0.135,Time Taken: 69.75550508499146 seconds.\n",
            "\n",
            "Epoch[77/100], Average Loss:  0.139,Time Taken: 70.84383010864258 seconds.\n",
            "\n",
            "Epoch[78/100], Average Loss:  0.139,Time Taken: 69.4863646030426 seconds.\n",
            "\n",
            "Epoch[79/100], Average Loss:  0.138,Time Taken: 70.09152317047119 seconds.\n",
            "\n",
            "Epoch[80/100], Average Loss:  0.135,Time Taken: 70.00623655319214 seconds.\n",
            "\n",
            "Epoch[81/100], Average Loss:  0.128,Time Taken: 69.10057163238525 seconds.\n",
            "\n",
            "Epoch[82/100], Average Loss:  0.128,Time Taken: 70.41418409347534 seconds.\n",
            "\n",
            "Epoch[83/100], Average Loss:  0.128,Time Taken: 68.29039359092712 seconds.\n",
            "\n",
            "Epoch[84/100], Average Loss:  0.127,Time Taken: 70.29789900779724 seconds.\n",
            "\n",
            "Epoch[85/100], Average Loss:  0.118,Time Taken: 69.49910402297974 seconds.\n",
            "\n",
            "Epoch[86/100], Average Loss:  0.125,Time Taken: 70.1491265296936 seconds.\n",
            "\n",
            "Epoch[87/100], Average Loss:  0.119,Time Taken: 69.18072867393494 seconds.\n",
            "\n",
            "Epoch[88/100], Average Loss:  0.124,Time Taken: 70.82633757591248 seconds.\n",
            "\n",
            "Epoch[89/100], Average Loss:  0.120,Time Taken: 68.6743426322937 seconds.\n",
            "\n",
            "Epoch[90/100], Average Loss:  0.119,Time Taken: 70.93977546691895 seconds.\n",
            "\n",
            "Epoch[91/100], Average Loss:  0.118,Time Taken: 68.77703785896301 seconds.\n",
            "\n",
            "Epoch[92/100], Average Loss:  0.114,Time Taken: 68.97136688232422 seconds.\n",
            "\n",
            "Epoch[93/100], Average Loss:  0.115,Time Taken: 71.07892346382141 seconds.\n",
            "\n",
            "Epoch[94/100], Average Loss:  0.122,Time Taken: 68.9976065158844 seconds.\n",
            "\n",
            "Epoch[95/100], Average Loss:  0.114,Time Taken: 69.8189160823822 seconds.\n",
            "\n",
            "Epoch[96/100], Average Loss:  0.114,Time Taken: 69.59339380264282 seconds.\n",
            "\n",
            "Epoch[97/100], Average Loss:  0.117,Time Taken: 71.09553575515747 seconds.\n",
            "\n",
            "Epoch[98/100], Average Loss:  0.107,Time Taken: 69.63172054290771 seconds.\n",
            "\n",
            "Epoch[99/100], Average Loss:  0.110,Time Taken: 69.33303380012512 seconds.\n",
            "\n",
            "Epoch[100/100], Average Loss:  0.103,Time Taken: 69.0984480381012 seconds.\n",
            "\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001,  betas=(0.95, 0.99))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):  # number of epochs\n",
        "    start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        # print(f'Epoch [{epoch +1}/{100}], Batch[{i+1}/{len(train_loader)}], Loss:{loss.item(): .3f}')\n",
        "        if i == len(train_loader) - 1:\n",
        "            end_time = time.time()\n",
        "            time_taken = end_time - start_time\n",
        "            print(f'Epoch[{epoch +1}/{100}], Average Loss: {running_loss/(i+1): .3f},Time Taken: {time_taken} seconds.\\n')\n",
        "            running_loss = 0.0\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKogGHz75Itp",
        "outputId": "8e089ba1-6138-4d42-8685-a069db51ca99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acccuracy: 45.77%\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    images,labels = data\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Acccuracy: {100*correct/total}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}